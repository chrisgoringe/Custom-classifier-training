# Train your own aesthetic model

This repository contains the scripts you need to train your own aesthetic scoring model in a couple of hours.

## Overview of the process

There are three main phases of the training. 

_One_, you gather several hundred images - the wider a range of subjects you want your `model` to work for, the more images you're likely to need. In testing I've worked with photorealistic(ish) images of people generated by ComfyUI - so with a random prompt generator it's easy to generate as many images as I need. But clearly the `model` you end up with is going to be of limited use on images which are totally different to those you trained with. Fortunately you can always add more images and do some more training without starting again.

_Two_, you use the _aesthetic_ab_scorer.py_ script to pick your preferred image from pairs which are offered to you. This script allows you to make these comparisons quickly (I do about 40 per minute) - don't overthink it! These choices train the `database`, which allocates a score to each image on the basis of your responses. At the end of each batch of comparisons (typically 100 pairs of images) you can see what proportion of your choice matched what the `database` would have predicted - this will start around 50% and increase over time as the `database` sorts out your order of preference. You roughly need to do about 3 times as many comparisons as there are images in the set, so this will take from about 30 minutes to an hour or two.

_Three_, you train a `model` using the _aesthetic_predictor_training.py_ script to reproduce the ordering of images in the `database`. The `model` is relatively simple, and will train in a matter of minutes. The resulting `model` can be used to give a score to any new image it sees.

### An iterative refinement

If you have a lot of images - or want to add new ones - you can refine the process by starting with a representative subset of the images and going through the above process. Then add the rest of the images (or another batch), and run the _aesthetic_score_from_model.py_ script, which uses the trained `model` to give an initial estimate score to all the new images. When you then run the _aesthetic_ab_scorer.py_ it weights its choice of images to show you to favour those which have not been seen as often, so it will tend towards the new ones, but it starts with a reasonable guess about your preferences, so requires fewer comparisons to find the right ordering.

## Some notes worth reading

### 75% is a good score

Human aesthetic preference is not linear and stable! Psychology research in the area suggests that the reproducability of an AB aesthetic preference over a period of days is about 75% - a quarter of the time, when shown the same pair of (relatively similar) images, you'll make the opposite choice to the one you made before! This excludes obvious outliers (people with three legs, for instance), but it does indicate that if you get the proportion of choices matching the `database` up to 75-80% that's probably as good as you are going to get. Similarly, it suggests that if your `model` can get a score of 75%+, it's probably going to be almost as good as you at choosing. The _aesthetic_ab_scorer.py_ script has the option to evaluate image pairs with a `model` that you have trained and show some statistics on which of the three choice - yours, the `database` prediction, and the `model` score, is the odd-one-out when they don't all agree. This can hint at whether the model is good enough.

### The model is easily overtrained

If the `model` isn't good enough, more training is probably not the answer, and more AB scoring probably isn't either (assuming your agreement with the `database` is up at the 75-80% mark). It'll need more images to be added to the set, to help it generalise better.

### You want maths? We've got maths

The update to the database scores for images is based on the ELO scoring system (used to rank chess players, among other things). When two images are compared, the probability that A will be prefered to B is given by `P(A>B) = 1 / (1+10^-delta)`, where delta is difference between the current rating of the two images. When the comparison takes place, the ratings of each are updated by calculating the unlikeliness of the result (`1-P`), multiplying it by a constant `K`. This value, `K(1-P)`, is added to the rating of the preferred image, and subtracted from that of the other. Since the model is trained on comparisons of values, not absolute values, the actual value of K doesn't matter (but it's 0.8, because reasons).

The model is trained by default using a MarginRankingLoss, which essentially rates the model on how accurately it reconstructs the ordering of images, rather than the absolute values of their scores. For this reason, both the database and model outputs should be rescaled before use; I have adopted the convention of rescaling them to a mean value of zero and standard deviation of one (with that mean and standard deviation calculated across the whole set of training images).

## A step-by-step recipe

### Gathering images

folder structure, names of files generated

### Running AB testing

meaning of stats generated


after model has been trained, human odd-one-outs go up (because model close to database).
as you do ABs and improve database, human odd-one-outs should go down and model up


you're trying to pull the model towards the human. AB training pulls the DB towards the human, model training pulls the model towards the DB.

So roughly: human odd-one-out > model odd-one-out means you need more AB; the other way around means you might consider training

### Training

metadatasearch

### Using

a script to use it, or Comfy nodes