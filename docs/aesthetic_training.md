# Train your own aesthetic model

This repository contains the scripts you need to train your own aesthetic scoring model in a couple of hours.

## Overview of the process

There are three main phases of the training. This is an overview, there is more detail below.

_One_, you gather several hundred images - the wider a range of subjects you want your `model` to work for, the more images you're likely to need. In testing I've worked with photorealistic(ish) images of people generated by ComfyUI - so with a random prompt generator it's easy to generate as many images as I need. But clearly the `model` you end up with is going to be of limited use on images which are totally different to those you trained with. Fortunately you can always add more images and do some more training without starting again.

_Two_, you use the _aesthetic_ab_scorer.py_ script to pick your preferred image from pairs which are offered to you. This script allows you to make these comparisons quickly (I do about 40 per minute) - don't overthink it! These choices train the `database`, which allocates a score to each image on the basis of your responses. At the end of each batch of comparisons (typically 100 pairs of images) you can see what proportion of your choice matched what the `database` would have predicted - this will start around 50% and increase over time as the `database` sorts out your order of preference. You roughly need to do about 3 times as many comparisons as there are images in the set, so this will take from about 30 minutes to an hour or two.

_Three_, you train a `model` using the _aesthetic_predictor_training.py_ script to reproduce the ordering of images in the `database`. The `model` is relatively simple, and will train in a matter of minutes. The resulting `model` can be used to give a score to any new image it sees.

### An iterative refinement

If you have a lot of images - or want to add new ones - you can refine the process by starting with a representative subset of the images and going through the above process. Then add the rest of the images (or another batch), and run the _aesthetic_score_from_model.py_ script, which uses the trained `model` to give an initial estimate score to all the new images. When you then run the _aesthetic_ab_scorer.py_ it weights its choice of images to show you to favour those which have not been seen as often, so it will tend towards the new ones, but it starts with a reasonable guess about your preferences, so requires fewer comparisons to find the right ordering.

I find this way of thinking helpful: the general problem is "which of these two images is preferred?", and the aim of the process is to get a `model` which will tend to give the same answer as you would give. Because training a `model` requires that question to be answered many, many thousands of times, we use a `database` to answer it in your place. The AB  process moves the `database` closer to your answers, training the `model` moves the `model` closer to the `database`. 

Adding new images to the `database` moves the `database` randomly, and using the model to initialise their values moves the `database` towards the `model`.

So when new images are added, the `database` then needs to be moved towards your preferences by AB training, and then the `model` can be trained to bring it closer to the `database`.

## Some notes worth reading

### 75% is a good score

Human aesthetic preference is not linear and stable! Psychology research in the area suggests that the reproducability of an AB aesthetic preference over a period of days is about 75% - a quarter of the time, when shown the same pair of (relatively similar) images, you'll make the opposite choice to the one you made before! This excludes obvious outliers (people with three legs, for instance), but it does indicate that if you get the proportion of choices matching the `database` up to 75-80% that's probably as good as you are going to get. Similarly, it suggests that if your `model` can get a score of 75%+, it's probably going to be almost as good as you at choosing. The _aesthetic_ab_scorer.py_ script has the option to evaluate image pairs with a `model` that you have trained and show some statistics on which of the three choice - yours, the `database` prediction, and the `model` score, is the odd-one-out when they don't all agree. This can hint at whether the model is good enough.

### The model is easily overtrained

If the `model` isn't good enough, more training is probably not the answer, and more AB scoring probably isn't either (assuming your agreement with the `database` is up at the 75-80% mark). It'll need more images to be added to the set, to help it generalise better.

### Aesthetic models aren't good at details



### Database score calculation

The update to the database scores for images is based on the ELO scoring system (used to rank chess players, among other things). When two images are compared, the probability that A will be prefered to B is given by `P(A>B) = 1 / (1+10^-delta)`, where delta is difference between the current rating of the two images. When the comparison takes place, the ratings of each are updated by calculating the unlikeliness of the result (`1-P`), multiplying it by a constant `K`. This value, `K(1-P)`, is added to the rating of the preferred image, and subtracted from that of the other. Since the model is trained on comparisons of values, not absolute values, the actual value of K doesn't matter (but it's 0.8, because reasons).

The model is trained by default using a MarginRankingLoss, which essentially rates the model on how accurately it reconstructs the ordering of images, rather than the absolute values of their scores. For this reason, both the database and model outputs should be rescaled before use; I have adopted the convention of rescaling them to a mean value of zero and standard deviation of one (with that mean and standard deviation calculated across the whole set of training images).

## A step-by-step recipe

### Gathering images

folder structure, names of files generated

### Running AB training

AB training is best done in blocks of comparison - 100 seems a good number - as statistics generated during the training can be helpful to see how you are getting on. When you run the _aesthetic_ab_scorer.py_ script it produces `ab_stats.txt` which contains lines like one of these:

```
all agree  64 ; model-choice agree  72 ; db-choice agree  75 ; db-model agree  81     (if you have a part trained model)
  9684 tests: 75.00%     (if you don't)
```

The figure of 75% (or the 75 in the longer line - that was a test of 100 comparisons) is the fraction of the time that your choice matched the preference ordering in the database (db-choice agree). As mentioned above, if you get into th 75-80% range, you probably won't improve the database much more due to human variation (when you see images which are similar in quality, your choice might depend on many external factors).

If you also have a part-trained model you also get stats for model-choice agreement and db-model agreement. If you add new images (and intitialise them with the model) you will find that db-model agreement goes up (obviously), and db-choice may well go down (because the new images have only been assessed by the model, not by you). You then do ABs until db-choice gets back up to 75-80%, at which point you can use the database to train the model again. 

At the end of each run you also get, in the console, this:
```
 100 comparisons in  153.2 s
2/1661 of the images have no comparisons yet
 75 choices matched db,  25 contradicted db [  0 not predicted] = (75.00%)
 ```
The first line is pretty obvious. The second shows you how many of your images have never been manually compared - this would ideally be zero. The script preferentially shows images that have been shown less frequently. The third line is the db-choice agreement.


### Training

metadatasearch

### Spotlight



## Using your model

a script to use it, or Comfy nodes